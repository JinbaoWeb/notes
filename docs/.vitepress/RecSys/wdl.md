# Wide & Deep：同时拥有“记忆”与“泛化”的推荐系统架构

Google 的 Wide & Deep 模型是推荐系统领域的重要里程碑，它把线性模型的“记忆能力”和深度模型的“泛化能力”结合在一起，并通过联合训练实现互补增强。在 Google Play 的大规模在线系统中，这个架构带来了显著的业务提升，后来也成为 CTR、排序模型中的经典结构。

## 1. 引言

推荐系统的核心挑战之一，是同时面对：

* **高维稀疏特征**：如用户安装过的应用、地理位置、时间段、上下文类别等；
* **大量未见过的组合**：许多 “用户 x 内容” 的组合从未出现在训练集中；
* **强规则性特征**：某些特定组合具有显著关联，例如特定兴趣用户对某些内容更敏感。

传统的线性模型擅长记忆显式规则，但无法自动泛化；深度神经网络通过 embedding 和多层感知机可以自动捕捉潜在语义，但可能忽略一些稀疏而关键的“例外规律”。Wide & Deep 的提出，就是为了同时获得二者的能力。

## 2. 模型结构

Wide & Deep 是一个双分支结构，将 Wide（线性部分）与 Deep（深度部分）同时接入预测层，并通过同一个损失函数联合训练。完整结构如下：
![wdl.png](https://cdn.jsdelivr.net/gh/JinbaoSite/note-gen-image-sync@main/14ea3c2d-df9b-45a2-be5a-6039fa5422df.png)

* **Wide 部分**：线性模型，包含原始特征与人工筛选的特征交叉；
* **Deep 部分**：embedding + 多层全连接网络，用于捕捉特征的潜在关系；
* **联合输出**：两个分支的 logit 相加，再进入 sigmoid 得到最终预测。

最终预测形式为：

$$
\text{logit}(x) = y_{\mathrm{wide}}(x) + y_{\mathrm{deep}}(x) + b,
$$

$$
\hat{y}=\sigma(\text{logit}(x)).
$$

### 2.1 Wide 结构

Wide 部分是一个扩展的广义线性模型，其关键在于手工构造的交叉特征。

假设输入特征为二值向量：

$$
x = [x_1, x_2, \ldots, x_d].
$$

定义交叉特征为：

$$
\phi_k(x)=\prod_{i=1}^d x_i^{c_{k,i}},\qquad c_{k,i}\in{0,1}.
$$

如果一个交叉项涉及的特征集合为 ($S_k$)，则只有所有 ($i\in S_k$) 的特征都为 1 时，($\phi_k(x)=1$)。Wide 的输出为：

$$
y_{\mathrm{wide}} = w^\top [x;\phi(x)].
$$

由于 Wide 是纯线性形式，它可以显式记住稀疏但强规律性的特征组合。

#### 损失对 Wide 权重的梯度

使用二分类交叉熵损失：

$$
\mathcal{L}=-\left(y\log\hat{y}+(1-y)\log(1-\hat{y})\right),
\quad \hat{y}=\sigma(\text{logit}).
$$

logistic 回归的性质使得：

$$
\frac{\partial \mathcal{L}}{\partial \text{logit}} = \hat{y}-y.
$$

因此：

$$
\frac{\partial \mathcal{L}}{\partial w} = (\hat{y}-y)\cdot [x;\phi(x)].
$$

这说明 Wide 权重的更新完全由稀疏特征驱动，非常适合稀疏优化方法（如 FTRL）。

### 2.2 Deep 结构

Deep 分支由 embedding 和多层前馈网络构成。类别特征经过 embedding 映射到稠密向量，再与连续特征拼接进入网络。

令 embedding 层输出为：

$$
a^{(0)}=\mathrm{concat}({e^{(f)}},z),
$$

其中 ($e^{(f)}$) 是某个类别特征的 embedding，($z$) 是连续特征。

多层网络的前向为：

$$
a^{(l+1)}=f(W^{(l)}a^{(l)}+b^{(l)}),
$$

最终输出 deep logit：

$$
y_{\mathrm{deep}}=w_{\mathrm{deep}}^\top a^{(L)}.
$$

#### 深度网络的反向传播推导

从最终层开始：

$$
\delta^{(L)} = \frac{\partial \mathcal{L}}{\partial a^{(L)}} = (\hat{y}-y)\cdot w_{\mathrm{deep}}.
$$

对于任意中间层：

$$
\delta^{(l)}=(W^{(l)})^\top (\delta^{(l+1)}\odot f'(z^{(l)})).
$$

对某层 $W^{(l)}$ 的梯度为：

$$
\frac{\partial \mathcal{L}}{\partial W^{(l)}}=(\delta^{(l+1)}\odot f'(z^{(l)}))(a^{(l)})^\top.
$$

#### embedding 的梯度

embedding 的梯度只作用在被实际索引到的列上：

$$
\frac{\partial \mathcal{L}}{\partial E^{(f)}}=g_{e^{(f)}}(x^{(f)})^\top.
$$

若 $x^{(f)}$ 是 one-hot，则只有一列被更新；若是 multi-hot，则多列会累加更新。

Deep 的梯度是密集的，并经过多层传播，这使得它适合用 AdaGrad 或 Adam 一类的自适应优化器。

### 2.3 联合训练

Wide 与 Deep 的 logit 在训练时直接相加，共享同一个损失函数：

$$
\text{logit}=y_{\mathrm{wide}}+y_{\mathrm{deep}}+b.
$$

因此其梯度共享同一个关键因子：

$$
\frac{\partial \mathcal{L}}{\partial \text{logit}}=\hat{y}-y.
$$

这意味着：

* Wide 和 Deep 会同时根据误差方向被更新；
* Deep 捕捉泛化信息，Wide 记录“例外规律”；
* Wide 自动补偿 Deep 的不足，Deep 自动平滑 Wide 的规则。

这种互补方式是 Wide & Deep 在工业场景表现优于单模型的重要原因。

## 3. 系统实现

![wdl_pipeline.png](https://cdn.jsdelivr.net/gh/JinbaoSite/note-gen-image-sync@main/cef32554-a117-4597-b07d-71e31c910d5b.png)

Wide & Deep 在 Google Play 的成功，很大程度上源于它的工程化能力：能在万亿级特征空间、千亿级样本量、毫秒级推理延迟下稳定运行。

### 3.1 数据处理

数据模块是 Wide & Deep 成功的基础。模型虽然能通过联合训练自动学习很多关系，但输入数据决定了上限。数据处理主要包含四个部分：**特征收集、类别特征词表构建、连续特征归一化、特征交叉构建**。

#### 3.1.1 类别特征的收集与词表（Vocabulary）构建

推荐系统中大量信息来自类别特征，例如：

* 用户已安装 App 的集合
* 当前展示的 App 或内容 ID
* 上下文离散特征（时段、国家、设备型号等）
* 用户行为序列中的离散事件

类别特征具有典型的长尾分布：极少数 token 高频出现，大部分 token 非常稀疏。如果将所有 token 全量纳入 embedding，会导致：

* 参数过大；
* embedding 更新不稳定；
* 无意义的低频 token 噪声会干扰模型收敛。

因此需要构建词表，选择频率超过阈值的 token（如超过 100 次）加入 embedding，其他 token 归入一个统一的 “OOV” 类别。

词表构建流程通常包括以下步骤：

1. 对每类离散特征统计出现频率；
2. 按频率排序并设定截断阈值；
3. 为高频 token 分配唯一 embedding ID；
4. 将其余低频 token 映射到 OOV。

这种策略能够控制模型大小，同时保持稳健性。

#### 3.1.2 连续特征的分位数归一化（Quantile Normalization）

连续特征（如用户行为次数、内容评分、停留时长）常呈重尾分布，直接输入模型会导致梯度剧烈波动，进而影响 wide 与 deep 的梯度协调。

Wide & Deep 论文推荐使用分位数归一化，而非简单的 min-max 或标准化。

分位数归一化步骤：

1. 根据训练集的分布计算分位点（例如 1000 个分位段）；
2. 根据特征值所在分位位置，将其映射到 ([0,1]) 区间；
3. 对所有连续特征进行同样的量化与归一化。

优势：

* 对极端值（outlier）非常鲁棒；
* 让特征分布更均匀；
* 提高训练的稳定性，尤其对深度部分梯度影响明显；
* 在在线系统中易维护，因为分位点可以定期更新。

#### 3.1.3 特征交叉构建（Wide 分支）

Wide 分支依赖特征交叉来显式建模某些组合关系。例如：

* 用户已安装 A 类音乐 App，会提升对 B 类音乐 App 的倾向；
* 某设备型号对某类广告有不同的点击偏好；
* 特定国家 + 特定内容类别有特殊相关性。

构建交叉特征时要考虑：

1. 不能做所有组合（组合爆炸）；
2. 必须基于业务知识和统计指标；
3. 可结合互信息、卡方检验等筛选价值高的组合；
4. 对非常高维的交叉空间可采用哈希技巧降低维度。

在工业系统中，这类显式交叉特征往往对应业务逻辑的“硬规则”，能让 Wide 分支在某些极小众但关键的场景中非常可靠。

### 3.2 模型训练

训练部分是 Wide & Deep 的核心，其难度在于：需要**同时训练稀疏的 Wide 参数**和**稠密的 Deep 参数**，并在海量数据中保持模型稳定、快速收敛。


#### 3.2.1 不同优化器的分支更新策略

Google Play 的 Wide & Deep 采用了“分支区分优化策略”的方案：

##### Wide 分支：FTRL + L1 正则

Wide 分支的特征是高维稀疏，参数量巨大且多数位置常常为 0，因此使用 **FTRL（Follow-The-Regularized-Leader）** 非常合适。

优点：

* L1 正则会自动将很多权重推为 0，使模型天然稀疏；
* 对稀疏特征友好，拥有高效的在线更新能力；
* 对输入稀疏但频繁变化的推荐环境十分稳定。

在 CTR 广告系统、搜索排序系统中，FTRL 是事实上的工业标准。

##### Deep 分支：AdaGrad（或 Adam）

Deep 分支的参数包括：

* embedding；
* 多层全连接网络的权重；
* Deep logit 的权重。

这些参数是稠密的，需要更平滑、稳定的学习率，因此使用自适应优化器 AdaGrad（或 Adam）更合适。

AdaGrad 能自动根据历史梯度调整每个坐标的学习率，使稀疏激活的 embedding 得到更大步长，实现高效训练。

#### 3.2.2 样本量巨大的分布式训练

Google Play 的训练数据每次可能涉及数百亿样本（论文中提到超过 500B 次事件）。为了支持如此数据规模，必须使用分布式训练。

分布式训练特征：

* 参数服务器架构将 embedding 表分散在多个节点；
* Worker 节点拉取 embedding 片段并执行前向与后向传播；
* 可以进行模型并行（embedding 存在 PS）与数据并行（多个 worker）；
* Deep 网络参数通常较小，可做全量同步或异步更新；
* Wide 参数可能非常大（数亿到数十亿维），稀疏更新极其关键。

#### 3.2.3 Warm-start：工业系统的必需品

Wide & Deep 的训练周期通常较长，从零开始训练成本高、时间不可控。因此使用 warm-start：

* 使用上一版本模型的 embedding 和权重作为起点；
* 新数据只需要增量学习；
* 若词表未发生变化，warm-start 可极大加速训练；
* 如果词表扩展，需要将旧 embedding 映射或复制到新 embedding 空间。

常见策略：

* 保持主词表稳定，不随时间频繁变化；
* 将低频 token 合并至 OOV，减少词表抖动；
* 每隔数周或数月才进行一次词表重构。

#### 3.2.4 Wide & Deep 在联合训练中的梯度协调

由于 Wide 分支和 Deep 分支的梯度尺度不同，实际训练时需要注意：

* 连续特征归一化防止 Deep 梯度爆炸；
* Wide 分支通过 L1 稀疏化参数，可防止模型复杂度失控；
* 深度网络通常用较小学习率，而 FTRL 会根据宽度自动调节；
* 在联合 logit 下，两分支共享同一个误差项，有助于模型平衡。

这一协调机制是 Wide & Deep 效果优异的根源。

### 3.3 模型服务（在线推理）

模型服务的目标是：**在极短延迟内对若干候选样本计算得分**。Google Play 的要求通常在毫秒级，因此需要强工程优化。

以下从并行推理、候选筛选、模型压缩等角度展开。

#### 3.3.1 推理阶段的多线程小批量化（micro-batching）

在线推理通常处理每次请求的几十到几百个候选。Single thread 性能不够，因此需要 micro-batching：

* 将候选切割成多个更小的 batch；
* 多线程并行执行；
* 大幅提升吞吐与降低延迟。

单线程批处理约为 31ms，多线程并行可降低至 14ms 左右，接近 50% 的加速。

#### 3.3.2 两阶段架构：召回 → 排序

Wide & Deep 通常作为排序模型，只对少量候选（如 100 个）做高精度打分。完整流程为：

1. **召回阶段**：使用轻模型或规则快速选取候选；
2. **排序阶段**：使用 Wide & Deep 对候选做精排序。

这种架构可显著降低 Wide & Deep 的调用次数，保证整体延迟。

#### 3.3.3 模型量化与参数压缩

为了减少内存与计算量，工程系统会：

* 将 embedding 压缩为低精度（如 16 bit 或 8 bit）；
* 对线性权重稀疏化（L1 已经自然带来稀疏效应）；
* 使用特殊格式存储 embedding，使加载更快；
* 在线服务节点会提前进行模型预热，避免首请求延迟。

#### 3.3.4 可靠性与可观测性

在推荐系统中，模型服务不仅需要快，还要可靠：

* 采用多层缓存，减少重复的 embedding lookup；
* 对服务节点进行健康检查，避免长尾延迟；
* 通过日志与指标监控跟踪 Wide 与 Deep 部分的训练和服务分布漂移。

工程上，这些可观测性措施与模型架构本身同等重要。

## 4. 总结

Wide & Deep 成功地将两类学习能力融合：

* **Wide：记忆**强规则与高频组合
* **Deep：泛化**潜在语义与未见组合
* **Joint Training：互补增强**

Wide 负责把特殊例外记住，Deep 负责理解整体结构，两者的梯度在同一损失下被共同优化，从而取得性能优于任何单模型的效果。

在 Google Play 的大规模在线实验中，Wide & Deep 显著提升了应用安装率，至今仍在推荐系统、CTR 预估、广告排序等任务中被广泛使用，并启发了后续一系列深度推荐模型。

## 5. 参考文献

* Heng-Tze Cheng et al., *Wide & Deep Learning for Recommender Systems*, Google, 2016.

